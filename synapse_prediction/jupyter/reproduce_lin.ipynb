{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenxupeng/projects/connectome\n"
     ]
    }
   ],
   "source": [
    "cd /home/chenxupeng/projects/connectome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append('EM-pytorch-master/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time, argparse, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "import os, sys; \n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  ????\n",
    "from em.model.io import load_checkpoint,save_checkpoint\n",
    "from em.model.unet import unet3D\n",
    "from em.model.deploy import unet3D_m1, unet3D_m2, unet3D_m2_v2\n",
    "from em.model.optim import decay_lr\n",
    "from em.model.loss import weightedMSE,malisWeight,labelWeight\n",
    "from em.data.volumeData import VolumeDatasetTrain, VolumeDatasetTest, np_collate\n",
    "from em.data.io import getVar, getImg, getLabel, cropCentralN\n",
    "from em.data.augmentation import DataAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Training Model')\n",
    "    # I/O\n",
    "    parser.add_argument('-m','--model-id',  type=float, default=0,\n",
    "                        help='model id')\n",
    "    parser.add_argument('-t','--train',  default='/n/coxfs01/donglai/malis_trans/data/ecs-3d/ecs-gt-3x6x6/',\n",
    "                        help='input folder (train)')\n",
    "    parser.add_argument('-v','--val',  default='',\n",
    "                        help='input folder (test)')\n",
    "    parser.add_argument('-dn','--img-name',  default='im_uint8.h5',\n",
    "                        help='image data')\n",
    "    parser.add_argument('-ln','--seg-name',  default='seg-groundtruth2-malis.h5',\n",
    "                        help='segmentation label')\n",
    "    parser.add_argument('-dnv','--img-name-val',  default='im_uint8.h5',\n",
    "                        help='image data for val')\n",
    "    parser.add_argument('-lnv','--seg-name-val',  default='seg-groundtruth2-malis.h5',\n",
    "                        help='segmentation label for val')\n",
    "    parser.add_argument('-dnd','--img-dataset-name',  default='main',\n",
    "                        help='dataset name in data')\n",
    "    parser.add_argument('-lnd','--seg-dataset-name',  default='main',\n",
    "                        help='dataset name in label')\n",
    "    parser.add_argument('-o','--output', default='result/train/',\n",
    "                        help='output path')\n",
    "    parser.add_argument('-s','--snapshot',  default='',\n",
    "                        help='pre-train snapshot path')\n",
    "\n",
    "    # model option\n",
    "    parser.add_argument('-ma','--opt-arch', type=str,  default='0,0@0@0,0,0@0',\n",
    "                        help='model type')\n",
    "    parser.add_argument('-mp','--opt-param', type=str,  default='0@0@0@0',\n",
    "                        help='model param')\n",
    "    parser.add_argument('-mi','--model-input', type=str,  default='31,204,204',\n",
    "                        help='model input size')\n",
    "    parser.add_argument('-mo','--model-output', type=str,  default='3,116,116',\n",
    "                        help='model input size')\n",
    "    parser.add_argument('-f', '--num-filter', default='24,72,216,648',\n",
    "                        help='number of filters per layer')\n",
    "    parser.add_argument('-ps', '--pad-size', type=int, default=0,\n",
    "                        help='pad size')\n",
    "    parser.add_argument('-pt', '--pad-type', default='constant,0',\n",
    "                        help='pad type')\n",
    "    parser.add_argument('-bn', '--has-BN', type=int, default=0,\n",
    "                        help='use BatchNorm')\n",
    "    parser.add_argument('-rs', '--relu-slope', type=float, default=0.005,\n",
    "                        help='relu type')\n",
    "    parser.add_argument('-do', '--has-dropout', type=float, default=0,\n",
    "                        help='use dropout')\n",
    "    parser.add_argument('-it','--init', type=int,  default=-1,\n",
    "                        help='model initialization type')\n",
    "\n",
    "    # data option\n",
    "    parser.add_argument('-ao','--aug-opt', type=str,  default='1@-1@0@5',\n",
    "                        help='data aug type')\n",
    "    parser.add_argument('-apw','--aug-param-warp', type=str,  default='15@3@1.1@0.1',\n",
    "                        help='data warp aug parameter')\n",
    "    parser.add_argument('-apc','--aug-param-color', type=str,  default='0.95,1.05@-0.15,0.15@0.5,2@0,1',\n",
    "                        help='data color aug parameter')\n",
    "\n",
    "    # optimization option\n",
    "    parser.add_argument('-l','--loss-opt', type=int, default=0,\n",
    "                        help='loss type')\n",
    "    parser.add_argument('-lw','--loss-weight-opt', type=float, default=2.0,\n",
    "                        help='weighted loss type')\n",
    "    parser.add_argument('-lr', type=float, default=0.0001,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('-lr_decay', default='inv,0.0001,0.75',\n",
    "                        help='learning rate decay')\n",
    "    parser.add_argument('-betas', default='0.99,0.999',\n",
    "                        help='beta for adam')\n",
    "    parser.add_argument('-wd', type=float, default=5e-6,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--volume-total', type=int, default=1000,\n",
    "                        help='total number of iteration')\n",
    "    parser.add_argument('--volume-save', type=int, default=100,\n",
    "                        help='number of iteration to save')\n",
    "    parser.add_argument('-e', '--pre-epoch', type=int, default=0,\n",
    "                        help='previous number of epoch')\n",
    "    parser.add_argument('-g','--num-gpu', type=int,  default=1,\n",
    "                        help='number of gpu')\n",
    "    parser.add_argument('-c','--num-cpu', type=int,  default=1,\n",
    "                        help='number of cpu')\n",
    "    parser.add_argument('-b','--batch-size', type=int,  default=1,\n",
    "                        help='batch size')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(args):\n",
    "    sn = args.output+'/'\n",
    "    if not os.path.isdir(sn):\n",
    "        os.makedirs(sn)\n",
    "    model_io_size = np.array([[int(x) for x in args.model_input.split(',')],\n",
    "                              [int(x) for x in args.model_output.split(',')]])\n",
    "\n",
    "    # pre-allocate torch cuda tensor for malis loss\n",
    "    train_vars = getVar(args.batch_size, model_io_size, [True, True, not (args.loss_opt == 0 and args.loss_weight_opt == 0)])\n",
    "    return model_io_size, train_vars\n",
    "\n",
    "def get_img(args, model_io_size, opt='train'):\n",
    "    # two dataLoader, can't be both multiple-cpu (pytorch issue)\n",
    "    if opt=='train':\n",
    "        dir_name = args.train.split('@')\n",
    "        num_worker = args.num_cpu\n",
    "        img_name = args.img_name.split('@')\n",
    "        seg_name = args.seg_name.split('@')\n",
    "    else:\n",
    "        dir_name = args.val.split('@')\n",
    "        num_worker = 1\n",
    "        img_name = args.img_name_val.split('@')\n",
    "        seg_name = args.seg_name_val.split('@')\n",
    "    img_dataset_name = args.img_dataset_name.split('@')\n",
    "    seg_dataset_name = args.seg_dataset_name.split('@')\n",
    "\n",
    "    # should be either one or the same as dir_name\n",
    "    seg_name = [dir_name[x]+seg_name[0] for x in range(len(dir_name))] \\\n",
    "            if len(seg_name) == 1 else [dir_name[x]+seg_name[x] for x in range(len(dir_name))]\n",
    "    img_name = [dir_name[x]+img_name[0] for x in range(len(dir_name))] \\\n",
    "            if len(img_name) == 1 else [dir_name[x]+img_name[x] for x in range(len(dir_name))]\n",
    "    seg_dataset_name = seg_dataset_name*len(dir_name) if len(seg_dataset_name) == 1 else seg_dataset_name\n",
    "    img_dataset_name = img_dataset_name*len(dir_name) if len(img_dataset_name) == 1 else img_dataset_name\n",
    "\n",
    "    if len(dir_name[0])==0: # don't load data\n",
    "        return None\n",
    "\n",
    "    # 1. load data\n",
    "    # make sure img and label have the same size\n",
    "    # assume img and label have the same center\n",
    "    suf_aff = '_aff'+args.opt_param\n",
    "    train_img = getImg(img_name, img_dataset_name)\n",
    "    train_label = getLabel(seg_name, seg_dataset_name, suf_aff)\n",
    "    train_img, train_label = cropCentralN(train_img, train_label)\n",
    "\n",
    "    # 2. get dataAug\n",
    "    aug_opt = [int(x) for x in args.aug_opt.split('@')]\n",
    "    aug_param_warp = [float(x) for x in args.aug_param_warp.split('@')]\n",
    "    aug_param_color = [[float(y) for y in x.split(',')] for x in args.aug_param_color.split('@')]\n",
    "    data_aug = DataAugment(aug_opt, aug_param_warp, aug_param_color)\n",
    "\n",
    "    # if malis, then need seg\n",
    "    do_seg = args.loss_opt==1 # need seg if do malis loss\n",
    "    import pdb; pdb.set_trace()\n",
    "    dataset = VolumeDatasetTrain(train_img, train_label, do_seg, np.inf, \\\n",
    "                                 model_io_size[0], model_io_size[1], data_aug=data_aug)\n",
    "    # to have evaluation during training (two dataloader), has to set num_worker=0\n",
    "    img_loader =  torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=args.batch_size, shuffle=True, collate_fn = np_collate,\n",
    "            num_workers=num_worker, pin_memory=True)\n",
    "    return img_loader\n",
    "\n",
    "def get_model(args, model_io_size):\n",
    "    # 1. get model\n",
    "    num_filter = [int(x) for x in args.num_filter.split(',')]\n",
    "    if args.model_id==0: # flexible framework\n",
    "        opt_arch = [[int(x) for x in y.split(',')] for y in  args.opt_arch.split('@')]\n",
    "        opt_param = [[int(x) for x in y.split(',')] for y in  args.opt_param.split('@')]\n",
    "        model = unet3D(filters=num_filter, opt_arch = opt_arch, opt_param = opt_param,\n",
    "                       has_BN = args.has_BN==1, has_dropout = args.has_dropout, relu_slope = args.relu_slope,\n",
    "                       pad_size = args.pad_size, pad_type= args.pad_type)\n",
    "    elif args.model_id==2: # _m2\n",
    "        model = unet3D_m2(filters=num_filter, has_BN = args.has_BN==1)\n",
    "    elif args.model_id==2.1: # _m2_v2\n",
    "        model = unet3D_m2_v2(filters=num_filter, has_BN = args.has_BN==1)\n",
    "    elif args.model_id==3: # _m3_iso\n",
    "        model = unet3D_m3(filters=num_filter, has_BN = args.has_BN==1)    \n",
    "\n",
    "\n",
    "    # 2. load previous model weight\n",
    "    pre_epoch = args.pre_epoch\n",
    "    if len(args.snapshot)>0:\n",
    "        cp = load_checkpoint(args.snapshot)\n",
    "        model.load_state_dict(cp['state_dict'])\n",
    "        if pre_epoch == 0:\n",
    "            pre_epoch = cp['epoch']\n",
    "        print '\\t continue to train from epoch '+str(pre_epoch)\n",
    "\n",
    "    # 3. get loss weight\n",
    "    conn_dims = [args.batch_size,3]+list(model_io_size[1])\n",
    "    if args.loss_opt == 0: # L2 training\n",
    "        loss_w = labelWeight(conn_dims, args.loss_weight_opt)\n",
    "    elif args.loss_opt == 1: # malis training\n",
    "        loss_w = malisWeight(conn_dims, args.loss_weight_opt)\n",
    "\n",
    "    return model, loss_w, pre_epoch\n",
    "\n",
    "def get_logger(args):\n",
    "    log_name = args.output+'/log'\n",
    "    log_name += ['_L2_','_malis_'][args.loss_opt]+str(args.lr)+ '_'+str(args.loss_weight_opt)\n",
    "    if len(args.snapshot)>0:\n",
    "        log_name += '_'+args.snapshot[:args.snapshot.rfind('.')] if '/' not in args.snapshot else args.snapshot[args.snapshot.rfind('/')+1:args.snapshot.rfind('.')]\n",
    "    logger = open(log_name+'.txt','w',0) # unbuffered, write instantly\n",
    "    return logger\n",
    "\n",
    "def get_optimizer(args, model, pre_epoch=0):\n",
    "    betas = [float(x) for x in args.betas.split(',')]\n",
    "    frozen_id = []\n",
    "    if args.model_id==0 and model.up[0].opt[0]==0: # hacked upsampling layer\n",
    "        for i in range(len(model.up)):\n",
    "            frozen_id +=  list(map(id,model.up[i].up._modules['0'].parameters()))\n",
    "    elif args.model_id==2: # hacked upsampling layer\n",
    "        for i in range(len(model.upS)):\n",
    "            frozen_id +=  list(map(id,model.upS[i]._modules['0'].parameters()))\n",
    "    frozen_params = filter(lambda p: id(p) in frozen_id, model.parameters())\n",
    "    rest_params = filter(lambda p: id(p) not in frozen_id, model.parameters())\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': rest_params},\n",
    "        {'params': frozen_params, 'lr':0.0, 'weight_decay':0.0, 'betas':[0.0, 0.0]}],\n",
    "        lr=args.lr, betas=betas, weight_decay=args.wd)\n",
    "\n",
    "    lr_decay = args.lr_decay.split(',')\n",
    "    for i in range(1,len(lr_decay)):\n",
    "        lr_decay[i] =  float(lr_decay[i])\n",
    "    if pre_epoch != 0:\n",
    "        decay_lr(optimizer, args.lr, pre_epoch-1, lr_decay[0], lr_decay[1], lr_decay[2])\n",
    "    return optimizer, lr_decay\n",
    "\n",
    "def forward(model, data, vars, loss_w, args):\n",
    "    y_pred = model(vars[0])\n",
    "    vars[1].data.copy_(torch.from_numpy(data[1]))\n",
    "    # Weighted (L2)\n",
    "    if args.loss_opt == 0 and args.loss_weight_opt != 0:\n",
    "        vars[2].data.copy_(torch.from_numpy(loss_w.getWeight(data[1])))\n",
    "    # Weighted (MALIS)\n",
    "    elif args.loss_opt == 1 and args.loss_weight_opt != 0:\n",
    "        vars[2].data.copy_(torch.from_numpy(loss_w.getWeight(y_pred.data.cpu().numpy(), data[1], data[2])))\n",
    "    return weightedMSE(y_pred, vars[1], vars[2])\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "\n",
    "    print '0. initial setup'\n",
    "    model_io_size, train_vars = init(args) \n",
    "\n",
    "    print '1. setup data'\n",
    "    #train_loader = get_img(args, model_io_size, 'train')\n",
    "    #test_loader = get_img(args, model_io_size, 'val') if args.val != '' else None\n",
    "\n",
    "    print '2. setup model'\n",
    "    model, loss_w, pre_epoch = get_model(args, model_io_size)\n",
    "    logger = get_logger(args)\n",
    "\n",
    "    print '3. setup optimizer'\n",
    "    optimizer, lr_decay = get_optimizer(args, model, pre_epoch)\n",
    "\n",
    "    print '4. start training'\n",
    "    if args.num_gpu>1: model = nn.DataParallel(model, range(args.num_gpu))\n",
    "    model.cuda()\n",
    "    if args.lr == 0:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    # Normalize learning rate\n",
    "    # args.lr = args.lr * args.batch_size / 2\n",
    "    test_iter = test_loader.__iter__() if test_loader is not None else None\n",
    "    test_loss = 0\n",
    "    volume_id = pre_epoch\n",
    "    for iter_id, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        volume_id += args.batch_size\n",
    "\n",
    "        # copy data\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Training error\n",
    "        #print data[0].shape,data[1].shape,data[2].shape\n",
    "        #visSliceSeg(data[0], data[2], offset=[14,44,44],outN='tmp/train_seg'+str(iter_id)+'_'+str(data[3][0][0])+'.png', frame_id=0)\n",
    "        #visSliceSeg(data[0], data[1][0][1], offset=[14,44,44],outN='tmp/train_affy'+str(iter_id)+'_'+str(data[3][0][0])+'.png', frame_id=0)\n",
    "        #visSliceSeg(data[0], data[2], offset=[6,14,14],outN='result/db/train_'+str(iter_id)+'_'+str(data[3][0][0])+'.png', frame_id=0)\n",
    "        #visSlice(data[0][0,0],outN='result/db/train_im.png',frame_id=6)\n",
    "        train_vars[0].data.copy_(torch.from_numpy(data[0]))\n",
    "        train_loss = forward(model, data, train_vars, loss_w, args)\n",
    "\n",
    "        # Forward\n",
    "        t2 = time.time()\n",
    "        # Backward\n",
    "        if args.lr > 0:\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        t3 = time.time()\n",
    "        # Validation error\n",
    "        if test_iter is not None and iter_id % 5 == 0:\n",
    "            test_img = next(test_iter)\n",
    "            #visSliceSeg(test_img[0], test_img[2], offset=[14,44,44],outN='result/db/test_'+str(iter_id)+'_'+str(test_img[3][0][0])+'.png', frame_id=0)\n",
    "            train_vars[0].data.copy_(torch.from_numpy(test_img[0]))\n",
    "            test_loss = forward(model, test_img, train_vars, loss_w, args).data[0]\n",
    "\n",
    "        # Print log\n",
    "        logger.write(\"[Volume %d] train_loss=%0.3f test_loss=%0.3f lr=%.5f ModelTime=%.2f TotalTime=%.2f\\n\" % (volume_id,train_loss.data[0],test_loss,optimizer.param_groups[0]['lr'],t3-t2,t3-t1))\n",
    "\n",
    "        # Save progress\n",
    "        if volume_id % args.volume_save <args.batch_size or volume_id >= args.volume_total:\n",
    "            save_checkpoint(model, args.output+('/volume_%d.pth' % (volume_id)), optimizer, volume_id)\n",
    "        # Terminate\n",
    "        if volume_id >= args.volume_total:\n",
    "            break\n",
    "\n",
    "        # LR update\n",
    "        if args.lr > 0:\n",
    "            decay_lr(optimizer, args.lr, volume_id, lr_decay[0], lr_decay[1], lr_decay[2])\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
