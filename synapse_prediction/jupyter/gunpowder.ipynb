{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from hpc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/.local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['rc', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML, Image\n",
    "from ipywidgets import interact,Dropdown,IntSlider,FloatRangeSlider, FloatSlider, RadioButtons\n",
    "rc('animation', html='html5')\n",
    "import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from beakerx import *\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import h5py\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整个过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = tuple(\n",
    "        Hdf5Source(\n",
    "            'sample_'+s+'_padded_20160501.aligned.filled.cropped.hdf',\n",
    "            datasets = {\n",
    "                ArrayKeys.RAW: 'volumes/raw',\n",
    "                ArrayKeys.GT_LABELS: 'volumes/labels/neuron_ids_notransparency',\n",
    "                ArrayKeys.GT_MASK: 'volumes/labels/mask',\n",
    "            }\n",
    "        ) +\n",
    "        Normalize() +\n",
    "        RandomLocation()\n",
    "        for s in ['A', 'B', 'C']\n",
    "    )\n",
    "\n",
    "    artifact_source = (\n",
    "        Hdf5Source(\n",
    "            'sample_ABC_padded_20160501.defects.hdf',\n",
    "            datasets = {\n",
    "                ArrayKeys.RAW: 'defect_sections/raw',\n",
    "                ArrayKeys.ALPHA_MASK: 'defect_sections/mask',\n",
    "            }\n",
    "        ) +\n",
    "        RandomLocation(min_masked=0.05, mask_array_key=ArrayKeys.ALPHA_MASK) +\n",
    "        Normalize() +\n",
    "        IntensityAugment(0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        ElasticAugment([4,40,40], [0,2,2], [0,math.pi/2.0]) +\n",
    "        SimpleAugment(transpose_only_xy=True)\n",
    "    )\n",
    "\n",
    "    snapshot_request = BatchRequest()\n",
    "    snapshot_request.add_array_request(ArrayKeys.LOSS_GRADIENT, (56,56,56))\n",
    "\n",
    "    batch_provider_tree = (\n",
    "        data_sources +\n",
    "        RandomProvider() +\n",
    "        ExcludeLabels([8094], ignore_mask_erode=12) +\n",
    "        ElasticAugment([4,40,40], [0,2,2], [0,math.pi/2.0], prob_slip=0.05,prob_shift=0.05,max_misalign=25) +\n",
    "        SimpleAugment(transpose_only_xy=True) +\n",
    "        GrowBoundary(steps=3, only_xy=True) +\n",
    "        AddGtAffinities(affinity_neighborhood) +\n",
    "        SplitAndRenumberSegmentationLabels() +\n",
    "        IntensityAugment(0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        DefectAugment(\n",
    "            prob_missing=0.03,\n",
    "            prob_low_contrast=0.01,\n",
    "            prob_artifact=0.03,\n",
    "            artifact_source=artifact_source,\n",
    "            contrast_scale=0.1) +\n",
    "        ZeroOutConstSections() +\n",
    "        IntensityScaleShift(2,-1) +\n",
    "        BalanceAffinityLabels() +\n",
    "        PreCache(\n",
    "            cache_size=10,\n",
    "            num_workers=5) +\n",
    "        Train(solver_parameters, use_gpu=0) +\n",
    "        Snapshot(every=10, output_filename='batch_{id}.hdf', additional_request=snapshot_request)\n",
    "    )\n",
    "\n",
    "    n = 10\n",
    "    print(\"Training for\", n, \"iterations\")\n",
    "\n",
    "    with build(batch_provider_tree) as minibatch_maker:\n",
    "        for i in range(n):\n",
    "            minibatch_maker.request_batch(request)\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "## DefectAugment\n",
    "class gunpowder.DefectAugment(intensities, prob_missing=0.05, prob_low_contrast=0.05, prob_artifact=0.0, prob_deform=0.0, contrast_scale=0.1, artifact_source=None, artifacts=None, artifacts_mask=None, deformation_strength=20, axis=0)[source]\n",
    "\n",
    "Augment intensity arrays section-wise with artifacts like missing sections, low-contrast sections, by blending in artifacts drawn from a separate source, or by deforming a section.\n",
    "\n",
    "- intensities (ArrayKey) – The key of the array of intensities to modify.\n",
    "- prob_missing (float) –\n",
    "- prob_low_contrast (float) –\n",
    "- prob_artifact (float) –\n",
    "- prob_deform (float) – Probabilities of having a missing section, low-contrast section, an artifact (see param artifact_source) or a deformed slice. The sum should not exceed 1. Values in missing sections will be set to 0.\n",
    "contrast_scale (float, optional) – By how much to scale the intensities for a low-contrast section, used if prob_low_contrast > 0.\n",
    "- (class (artifact_source) – BatchProvider, optional):A gunpowder batch provider that delivers intensities (via ArrayKey artifacts) and an alpha mask (via ArrayKey artifacts_mask), used if prob_artifact > 0.\n",
    "\n",
    "- artifacts (ArrayKey, optional) – The key to query artifact_source for to get the intensities of the artifacts.\n",
    "- artifacts_mask (ArrayKey, optional) – The key to query artifact_source for to get the alpha mask of the artifacts to blend them with intensities.\n",
    "- deformation_strength (int, optional) – Strength of the slice deformation in voxels, used if prob_deform > 0. The deformation models a fold by shifting the section contents towards a randomly oriented line in the section. The line itself will be drawn with a value of 0.\n",
    "- axis (int, optional) – Along which axis sections are cut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticAugment\n",
    "class gunpowder.ElasticAugment(control_point_spacing, jitter_sigma, rotation_interval, prob_slip=0, prob_shift=0, max_misalign=0, subsample=1)[source]\n",
    "Elasticly deform a batch. Requests larger batches upstream to avoid data loss due to rotation and jitter.\n",
    "\n",
    "- control_point_spacing (tuple of int) – Distance between control points for the elastic deformation, in voxels per dimension.\n",
    "- jitter_sigma (tuple of float) – Standard deviation of control point jitter distribution, in voxels per dimension.\n",
    "rotation_interval (tuple of two floats) – Interval to randomly sample rotation angles from (0, 2PI).\n",
    "- prob_slip (float) – Probability of a section to “slip”, i.e., be independently moved in x-y.\n",
    "- prob_shift (float) – Probability of a section and all following sections to move in x-y.\n",
    "- max_misalign (int) – Maximal voxels to shift in x and y. Samples will be drawn uniformly. Used if prob_slip + prob_shift > 0.\n",
    "- subsample (int) – Instead of creating an elastic transformation on the full resolution, create one subsampled by the given factor, and linearly interpolate to obtain the full resolution transformation. This can significantly speed up this node, at the expense of having visible piecewise linear deformations for large factors. Usually, a factor of 4 can savely by used without noticable changes. However, the default is 1 (i.e., no subsampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntensityAugment\n",
    "class gunpowder.IntensityAugment(array, scale_min, scale_max, shift_min, shift_max, z_section_wise=False)[source]\n",
    "Randomly scale and shift the values of an intensity array.\n",
    "\n",
    "- array (ArrayKey) – The intensity array to modify.\n",
    "- scale_min (float) –\n",
    "- scale_max (float) –\n",
    "- shift_min (float) –\n",
    "- shift_max (float) –\n",
    "The min and max of the uniformly randomly drawn scaling and shifting values for the intensity augmentation. Intensities are changed as:\n",
    "```\n",
    "a = a.mean() + (a-a.mean())*scale + shift\n",
    "```\n",
    "- z_section_wise (bool) – Perform the augmentation z-section wise. Requires 3D arrays and assumes that z is the first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleAugment\n",
    "class gunpowder.SimpleAugment(mirror_only=None, transpose_only=None)[source]\n",
    "Randomly mirror and transpose all Arrays and Points in a batch.\n",
    "\n",
    "Parameters:\t\n",
    "- mirror_only (list of int, optional) – If set, only mirror between the given axes. This is useful to exclude channels that have a set direction, like time.\n",
    "- transpose_only (list of int, optional) – If set, only transpose between the given axes. This is useful to limit the transpose to axes with the same resolution or to exclude non-spatial dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gunpowder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first test simple augmentation, try without installing gunpowder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleAugment(BatchFilter):\n",
    "    '''Randomly mirror and transpose all :class:`Arrays<Array>` and\n",
    "    :class:`Points` in a batch.\n",
    "    Args:\n",
    "        mirror_only (``list`` of ``int``, optional):\n",
    "            If set, only mirror between the given axes. This is useful to\n",
    "            exclude channels that have a set direction, like time.\n",
    "        transpose_only (``list`` of ``int``, optional):\n",
    "            If set, only transpose between the given axes. This is useful to\n",
    "            limit the transpose to axes with the same resolution or to exclude\n",
    "            non-spatial dimensions.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mirror_only=None, transpose_only=None):\n",
    "\n",
    "        self.mirror_only = mirror_only\n",
    "        self.transpose_only = transpose_only\n",
    "        self.mirror_mask = None\n",
    "        self.dims = None\n",
    "        self.transpose_dims = None\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self.dims = self.spec.get_total_roi().dims()\n",
    "\n",
    "        if self.mirror_only is None:\n",
    "            self.mirror_mask = [ True ]*self.dims\n",
    "        else:\n",
    "            self.mirror_mask = [ d in self.mirror_only for d in range(self.dims) ]\n",
    "\n",
    "        if self.transpose_only is None:\n",
    "            self.transpose_dims = list(range(self.dims))\n",
    "        else:\n",
    "            self.transpose_dims = self.transpose_only\n",
    "\n",
    "    def prepare(self, request):\n",
    "\n",
    "        self.total_roi = request.get_total_roi()\n",
    "\n",
    "        self.mirror = [\n",
    "            random.randint(0,1)\n",
    "            if self.mirror_mask[d] else 0\n",
    "            for d in range(self.dims)\n",
    "        ]\n",
    "\n",
    "        t = list(self.transpose_dims)\n",
    "        random.shuffle(t)\n",
    "        self.transpose = list(range(self.dims))\n",
    "        for o, n in zip(self.transpose_dims, t):\n",
    "            self.transpose[o] = n\n",
    "\n",
    "        logger.debug(\"mirror = \" + str(self.mirror))\n",
    "        logger.debug(\"transpose = \" + str(self.transpose))\n",
    "\n",
    "        reverse_transpose = [0]*self.dims\n",
    "        for d in range(self.dims):\n",
    "            reverse_transpose[self.transpose[d]] = d\n",
    "\n",
    "        logger.debug(\"downstream request = \" + str(request))\n",
    "\n",
    "        self.__transpose_request(request, reverse_transpose)\n",
    "        self.__mirror_request(request, self.mirror)\n",
    "\n",
    "        logger.debug(\"upstream request = \" + str(request))\n",
    "\n",
    "    def process(self, batch, request):\n",
    "\n",
    "        mirror = tuple(\n",
    "                slice(None, None, -1 if m else 1)\n",
    "                for m in self.mirror\n",
    "        )\n",
    "        # arrays\n",
    "        for (array_key, array) in batch.arrays.items():\n",
    "\n",
    "            array.data = array.data[mirror]\n",
    "            if self.transpose != (0,1,2):\n",
    "                array.data = array.data.transpose(self.transpose)\n",
    "        # points\n",
    "        total_roi_offset = self.total_roi.get_offset()\n",
    "        for (points_key, points) in batch.points.items():\n",
    "\n",
    "            for loc_id, syn_point in points.data.items():\n",
    "                # mirror\n",
    "                location_in_total_offset = np.asarray(syn_point.location) - total_roi_offset\n",
    "                syn_point.location = np.asarray([self.total_roi.get_end()[dim] - location_in_total_offset[dim]\n",
    "                                                 if m else syn_point.location[dim] for dim, m in enumerate(self.mirror)])\n",
    "                # transpose\n",
    "                if self.transpose != (0, 1, 2):\n",
    "                    syn_point.location = np.asarray([syn_point.location[self.transpose[d]] for d in range(self.dims)])\n",
    "\n",
    "                # due to the mirroring, points at the lower boundary of the ROI\n",
    "                # could fall on the upper one, which excludes them from the ROI\n",
    "                if not points.spec.roi.contains(syn_point.location):\n",
    "                    del points.data[loc_id]\n",
    "\n",
    "        # arrays & points\n",
    "        for collection_type in [batch.arrays, batch.points]:\n",
    "            for (type, collector) in collection_type.items():\n",
    "                logger.debug(\"total ROI: %s\"%self.total_roi)\n",
    "                logger.debug(\"upstream %s ROI: %s\"%(type, collector.spec.roi))\n",
    "                self.__mirror_roi(collector.spec.roi, self.total_roi, self.mirror)\n",
    "                logger.debug(\"mirrored %s ROI: %s\"%(type,collector.spec.roi))\n",
    "                self.__transpose_roi(collector.spec.roi, self.transpose)\n",
    "                logger.debug(\"transposed %s ROI: %s\"%(type,collector.spec.roi))\n",
    "\n",
    "\n",
    "    def __mirror_request(self, request, mirror):\n",
    "\n",
    "        for key, spec in request.items():\n",
    "            self.__mirror_roi(spec.roi, self.total_roi, mirror)\n",
    "\n",
    "    def __transpose_request(self, request, transpose):\n",
    "\n",
    "        for key, spec in request.items():\n",
    "            self.__transpose_roi(spec.roi, transpose)\n",
    "\n",
    "    def __mirror_roi(self, roi, total_roi, mirror):\n",
    "\n",
    "        total_roi_offset = total_roi.get_offset()\n",
    "        total_roi_shape = total_roi.get_shape()\n",
    "\n",
    "        roi_offset = roi.get_offset()\n",
    "        roi_shape = roi.get_shape()\n",
    "\n",
    "        roi_in_total_offset = roi_offset - total_roi_offset\n",
    "        end_of_roi_in_total = roi_in_total_offset + roi_shape\n",
    "        roi_in_total_offset_mirrored = total_roi_shape - end_of_roi_in_total\n",
    "        roi_offset = Coordinate(\n",
    "                total_roi_offset[d] + roi_in_total_offset_mirrored[d] if mirror[d] else roi_offset[d]\n",
    "                for d in range(self.dims)\n",
    "        )\n",
    "\n",
    "        roi.set_offset(roi_offset)\n",
    "\n",
    "    def __transpose_roi(self, roi, transpose):\n",
    "\n",
    "        offset = roi.get_offset()\n",
    "        shape = roi.get_shape()\n",
    "        offset = tuple(offset[transpose[d]] for d in range(self.dims))\n",
    "        shape = tuple(shape[transpose[d]] for d in range(self.dims))\n",
    "        roi.set_offset(offset)\n",
    "        roi.set_shape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try mirror!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(mirror_only=None,transpose_only=None,dims=3):\n",
    "    if mirror_only is None:\n",
    "        mirror_mask = [ True ]*dims\n",
    "    else:\n",
    "        mirror_mask = [ d in self.mirror_only for d in range(dims) ]\n",
    "\n",
    "    if transpose_only is None:\n",
    "        transpose_dims = list(range(dims))\n",
    "    else:\n",
    "        transpose_dims = transpose_only\n",
    "    return mirror_mask,transpose_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True] [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "mirror_mask,transpose_dims = setup(mirror_only=None,transpose_only=None,dims=3)\n",
    "print (mirror_mask,transpose_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirror_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims=3\n",
    "#问题np.random.randint(0,1)只能产生0？！\n",
    "# random.randint generate random axis?\n",
    "[np.random.randint(0,1) if mirror_mask[d] else 0 for d in range(dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_roi(self):\n",
    "    '''Get the union of all the ROIs.'''\n",
    "\n",
    "    total_roi = None\n",
    "    for specs_type in [self.array_specs, self.points_specs]:\n",
    "        for (_, spec) in specs_type.items():\n",
    "            if total_roi is None:\n",
    "                total_roi = spec.roi\n",
    "            else:\n",
    "                total_roi = total_roi.union(spec.roi)\n",
    "    return total_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = {}\n",
    "tt is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_roi():\n",
    "    skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(dims, request,transpose_dims):\n",
    "    #self.total_roi = request.get_total_roi()\n",
    "\n",
    "    mirror = [\n",
    "        random.randint(0,1)\n",
    "        if self.mirror_mask[d] else 0\n",
    "        for d in range(dims)\n",
    "    ]   #只能产生0？\n",
    "\n",
    "    t = list(transpose_dims)\n",
    "    random.shuffle(t)\n",
    "    transpose = list(range(dims))\n",
    "    for o, n in zip(transpose_dims, t):\n",
    "        transpose[o] = n\n",
    "\n",
    "    logger.debug(\"mirror = \" + str(mirror))\n",
    "    logger.debug(\"transpose = \" + str(transpose))\n",
    "\n",
    "    reverse_transpose = [0]*dims\n",
    "    for d in range(dims):\n",
    "        reverse_transpose[transpose[d]] = d\n",
    "\n",
    "    logger.debug(\"downstream request = \" + str(request))\n",
    "\n",
    "    self.__transpose_request(request, reverse_transpose)\n",
    "    self.__mirror_request(request, self.mirror)\n",
    "\n",
    "    logger.debug(\"upstream request = \" + str(request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple augment can be achieved by EM-segLib/em_segLib/transform.py!\n",
    "in this repo, only transpose_only_xy=True is used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntensityAugment\n",
    "class gunpowder.IntensityAugment(array, scale_min, scale_max, shift_min, shift_max, z_section_wise=False)[source]\n",
    "Randomly scale and shift the values of an intensity array.\n",
    "\n",
    "- array (ArrayKey) – The intensity array to modify.\n",
    "- scale_min (float) –\n",
    "- scale_max (float) –\n",
    "- shift_min (float) –\n",
    "- shift_max (float) –\n",
    "The min and max of the uniformly randomly drawn scaling and shifting values for the intensity augmentation. Intensities are changed as:\n",
    "```\n",
    "a = a.mean() + (a-a.mean())*scale + shift\n",
    "```\n",
    "- z_section_wise (bool) – Perform the augmentation z-section wise. Requires 3D arrays and assumes that z is the first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly scale and shift the values of an intensity array，\n",
    "- 可以对z方向做逐层的的augmentation，这时要求第一个dimension是z\n",
    "- 要求array是normalize过得，0到1之内的\n",
    "- 需要制定scale和shift分别的最大最小值，然后通过\n",
    "```\n",
    "np.random.uniform(low=self.shift_min, high=self.shift_max))\n",
    "```\n",
    "获得一个scale和shift的值。\n",
    "- 每个array进行如下操作：a.mean() + (a-a.mean())*scale + shift\n",
    "- 对于返回的array，大于1和小于0的分别赋值为1和0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar implementation!\n",
    "- scale is similar to DataProvider/python/data_augmentation.py class GreyAugment(DataAugment)!\n",
    "- DataProvider/python/data_augmentation.py class GreyAugment(DataAugment)的方案**可能更好**：\n",
    "```\n",
    "img *= 1 + (np.random.rand() - 0.5)*self.CONTRAST_FACTOR\n",
    "img += (np.random.rand() - 0.5)*self.BRIGHTNESS_FACTOR\n",
    "img = np.clip(img, 0, 1)\n",
    "img **= 2.0**(np.random.rand()*2 - 1)\n",
    "```\n",
    "- 该方案是从ELEKTRONN (http://elektronn.org/).借鉴来的，ELEKTRONN似乎是一个专门处理2D/3D large scale image的库，也可以用来做segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import map_coordinates, zoom\n",
    "def create_elastic_transformation(shape, control_point_spacing = 100, jitter_sigma = 10.0, subsample = 1):\n",
    "\n",
    "    dims = len(shape)\n",
    "    subsample_shape = tuple(max(1,int(s/subsample)) for s in shape)\n",
    "\n",
    "    try:\n",
    "        spacing = tuple((d for d in control_point_spacing))\n",
    "    except:\n",
    "        spacing = (control_point_spacing,)*dims\n",
    "    try:\n",
    "        sigmas = [ s for s in jitter_sigma ]\n",
    "    except:\n",
    "        sigmas = [jitter_sigma]*dims\n",
    "\n",
    "    control_points = tuple(\n",
    "            max(1,int(round(float(shape[d])/spacing[d])))\n",
    "            for d in range(len(shape))\n",
    "    )\n",
    "\n",
    "    # jitter control points\n",
    "    control_point_offsets = np.zeros((dims,) + control_points, dtype=np.float32)\n",
    "    for d in range(dims):\n",
    "        if sigmas[d] > 0:\n",
    "            control_point_offsets[d] = np.random.normal(scale=sigmas[d], size=control_points)\n",
    "    print (control_point_offsets.shape)\n",
    "    return upscale_transformation(control_point_offsets, subsample_shape, interpolate_order=3)\n",
    "def upscale_transformation(transformation, output_shape, interpolate_order=1):\n",
    "\n",
    "    input_shape = transformation.shape[1:]\n",
    "\n",
    "    # print(\"Upscaling control points\")\n",
    "    # print(\"\\tfrom               : \" + str(input_shape))\n",
    "    # print(\"\\tto                 : \" + str(output_shape))\n",
    "    # print(\"\\tinterpolation order: \" + str(interpolate_order))\n",
    "\n",
    "    dims = len(output_shape)\n",
    "    scale = tuple(float(s)/c for s,c in zip(output_shape, input_shape))\n",
    "\n",
    "    start = time.time()\n",
    "    scaled = np.zeros((dims,)+output_shape, dtype=np.float32)\n",
    "    for d in range(dims):\n",
    "        zoom(transformation[d], zoom=scale, output=scaled[d], order=interpolate_order)\n",
    "    # print(\"\\tupsampled in \" + str(time.time() - start) + \"s\")\n",
    "\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotation_transformation(shape, angle, subsample=1):\n",
    "\n",
    "    dims = len(shape)\n",
    "    subsample_shape = tuple(max(1,int(s/subsample)) for s in shape)\n",
    "    control_points = (2,)*dims\n",
    "\n",
    "    # map control points to world coordinates\n",
    "    control_point_scaling_factor = tuple(float(s-1) for s in shape)\n",
    "\n",
    "    # rotate control points\n",
    "    center = np.array([0.5*(d-1) for d in shape])\n",
    "\n",
    "    # print(\"Creating rotation transformation with:\")\n",
    "    # print(\"\\tangle : \" + str(angle))\n",
    "    # print(\"\\tcenter: \" + str(center))\n",
    "\n",
    "    control_point_offsets = np.zeros((dims,) + control_points, dtype=np.float32)\n",
    "    for control_point in np.ndindex(control_points):\n",
    "\n",
    "        point = np.array(control_point)*control_point_scaling_factor\n",
    "        center_offset = np.array([p-c for c,p in zip(center, point)], dtype=np.float32)\n",
    "        rotated_offset = np.array(center_offset)\n",
    "        rotated_offset[-2:] = rotate(center_offset[-2:], angle)\n",
    "        displacement = rotated_offset - center_offset\n",
    "        control_point_offsets[(slice(None),) + control_point] += displacement\n",
    "\n",
    "    return upscale_transformation(control_point_offsets, subsample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 10, 100, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_elastic_transformation(\n",
    "        (10,100,100),\n",
    "        control_point_spacing = 100,\n",
    "        #num_control_points = [3,10,10],\n",
    "        jitter_sigma = [0.3, 10, 10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __misalign(self, transformation):\n",
    "\n",
    "    num_sections = transformation[0].shape[0]\n",
    "\n",
    "    shifts = [Coordinate((0,0,0))]*num_sections\n",
    "    for z in range(num_sections):\n",
    "\n",
    "        r = random.random()\n",
    "\n",
    "        if r <= self.prob_slip:\n",
    "\n",
    "            shifts[z] = self.__random_offset()\n",
    "\n",
    "        elif r <= self.prob_slip + self.prob_shift:\n",
    "\n",
    "            offset = self.__random_offset()\n",
    "            for zp in range(z, num_sections):\n",
    "                shifts[zp] += offset\n",
    "\n",
    "    logger.debug(\"misaligning sections with \" + str(shifts))\n",
    "\n",
    "    dims = 3\n",
    "    bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n",
    "    bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n",
    "    logger.debug(\"min/max of transformation: \" + str(bb_min) + \"/\" + str(bb_max))\n",
    "\n",
    "    for z in range(num_sections):\n",
    "        transformation[1][z,:,:] += shifts[z][1]\n",
    "        transformation[2][z,:,:] += shifts[z][2]\n",
    "\n",
    "    bb_min = tuple(int(math.floor(transformation[d].min())) for d in range(dims))\n",
    "    bb_max = tuple(int(math.ceil(transformation[d].max())) + 1 for d in range(dims))\n",
    "    logger.debug(\"min/max of transformation after misalignment: \" + str(bb_min) + \"/\" + str(bb_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者使用了ElasticAugment([4,40,40], [0,2,2], [0,math.pi/2.0], prob_slip=0.05,prob_shift=0.05,max_misalign=25)\n",
    "```\n",
    "class gunpowder.ElasticAugment(control_point_spacing, jitter_sigma, rotation_interval, prob_slip=0, prob_shift=0, max_misalign=0, subsample=1)\n",
    "```\n",
    "- reshape array data into (channels,) + spatial dims\n",
    "- 首先 **create_identity_transformation**\n",
    "    - create_identity_transformation来自funkey的另一个repo augment，这个repo是之前的augment的简单方法，create_identity_transformation,从这里开始图像的channel会变成3,主要用np.meshgrid增加两个通道\n",
    "- 其次 如果sum(jitter_sigma) > 0: **create_elastic_transformation**\n",
    "```\n",
    "augment.create_elastic_transformation(\n",
    "                    target_shape,\n",
    "                    self.control_point_spacing,\n",
    "                    self.jitter_sigma,\n",
    "                    subsample=self.subsample)\n",
    "```\n",
    "    -先得到control_point_offsets，再插值做upscale，依然生成3 channel图片\n",
    "- 然后，由rotation_interval [0,math.pi/2.0], rotation_start = rotation_interval[0], rotation_max_amount = rotation_interval[1] - rotation_interval[0]\n",
    "\n",
    "    rotation = random.random()*self.rotation_max_amount + self.rotation_start(在作者设置中是一个小于90度的值)\n",
    "    \n",
    "    如果rotation>0,做**create_rotation_transformation**\n",
    "    \n",
    "以上步骤都是之前的augment repo中实现的方法，作者直接在gunpowder中使用了\n",
    "\n",
    "- 然后，如果prob_slip + prob_shift > 0，做**__misalign(transformation)**\n",
    "    \n",
    "    根据阈值做逐层的随机shift\n",
    "\n",
    "- 最后，如果subsample >1（作者使用的是1），在做elastic augmentation之前会做subsample，加快elasticaugment的计算速度，做完augmentation再upscale\n",
    "\n",
    "    Instead of creating an elastic transformation on the full\n",
    "    resolution, create one subsampled by the given factor, and linearly\n",
    "    interpolate to obtain the full resolution transformation. This can\n",
    "    significantly speed up this node, at the expense of having visible\n",
    "    piecewise linear deformations for large factors. Usually, a factor\n",
    "    of 4 can savely by used without noticable changes. However, the\n",
    "    default is 1 (i.e., no subsampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefectAugment\n",
    "class gunpowder.DefectAugment(intensities, prob_missing=0.05, prob_low_contrast=0.05, prob_artifact=0.0, prob_deform=0.0, contrast_scale=0.1, artifact_source=None, artifacts=None, artifacts_mask=None, deformation_strength=20, axis=0)[source]\n",
    "\n",
    "Augment intensity arrays section-wise with artifacts like missing sections, low-contrast sections, by blending in artifacts drawn from a separate source, or by deforming a section.\n",
    "\n",
    "- intensities (ArrayKey) – The key of the array of intensities to modify.\n",
    "- prob_missing (float) –\n",
    "- prob_low_contrast (float) –\n",
    "- prob_artifact (float) –\n",
    "- prob_deform (float) – Probabilities of having a missing section, low-contrast section, an artifact (see param artifact_source) or a deformed slice. The sum should not exceed 1. Values in missing sections will be set to 0.\n",
    "contrast_scale (float, optional) – By how much to scale the intensities for a low-contrast section, used if prob_low_contrast > 0.\n",
    "- (class (artifact_source) – BatchProvider, optional):A gunpowder batch provider that delivers intensities (via ArrayKey artifacts) and an alpha mask (via ArrayKey artifacts_mask), used if prob_artifact > 0.\n",
    "\n",
    "- artifacts (ArrayKey, optional) – The key to query artifact_source for to get the intensities of the artifacts.\n",
    "- artifacts_mask (ArrayKey, optional) – The key to query artifact_source for to get the alpha mask of the artifacts to blend them with intensities.\n",
    "- deformation_strength (int, optional) – Strength of the slice deformation in voxels, used if prob_deform > 0. The deformation models a fold by shifting the section contents towards a randomly oriented line in the section. The line itself will be drawn with a value of 0.\n",
    "- axis (int, optional) – Along which axis sections are cut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一个专门的artifect文件读取数据并做defectaugment\n",
    "\n",
    "数据如下，包括defects文件，raw和mask文件，并且在内部也做了一遍augmentation\n",
    "```\n",
    "artifact_source = (\n",
    "        Hdf5Source(\n",
    "            'sample_ABC_padded_20160501.defects.hdf',\n",
    "            datasets = {\n",
    "                ArrayKeys.RAW: 'defect_sections/raw',\n",
    "                ArrayKeys.ALPHA_MASK: 'defect_sections/mask',\n",
    "            }\n",
    "        ) +\n",
    "        RandomLocation(min_masked=0.05, mask_array_key=ArrayKeys.ALPHA_MASK) +\n",
    "        Normalize() +\n",
    "        IntensityAugment(0.9, 1.1, -0.1, 0.1, z_section_wise=True) +\n",
    "        ElasticAugment([4,40,40], [0,2,2], [0,math.pi/2.0]) +\n",
    "        SimpleAugment(transpose_only_xy=True)\n",
    "    )\n",
    "```\n",
    "- 作者设置的阈值：\n",
    "```\n",
    "DefectAugment(\n",
    "            prob_missing=0.03,\n",
    "            prob_low_contrast=0.01,\n",
    "            prob_artifact=0.03,\n",
    "            artifact_source=artifact_source,\n",
    "            contrast_scale=0.1)\n",
    "```\n",
    "- 分别获得missing，low_contrast,artifact,deform的值\n",
    "    prob_missing_threshold = self.prob_missing   (0.03=0.03)\n",
    "    prob_low_contrast_threshold = prob_missing_threshold + self.prob_low_contrast  (0.04=0.03+0.01)\n",
    "    prob_artifact_threshold = prob_low_contrast_threshold + self.prob_artifact    (0.07=0.03+0.04)\n",
    "    prob_deform_slice = prob_artifact_threshold + self.prob_deform   (0.07=0.07+0)\n",
    "- 对每一个slice，产生一个0-1的随机数r，如果：\n",
    "    - r < prob_missing_threshold:  'zero_out'\n",
    "        - 什么也不做，直接输出\n",
    "    - elif r < prob_low_contrast_threshold:  'lower_contrast'\n",
    "        - mean = section.mean(), section -= mean, section *= self.contrast_scale, section += mean\n",
    "    - elif r < prob_artifact_threshold:'artifact'\n",
    "        - raw.data[section_selector] = section*(1.0 - artifact_alpha) + artifact_raw*artifact_alpha\n",
    "        - artifact_alpha, artifact_raw来自artifact_source的mask和raw， 相当于做了一个Alpha Blending\n",
    "    - elif r < prob_deform_slice: 'deformed_slice' 如果是deformed slice, 需要bigger upstream roi for deformed slice\n",
    "        ```\n",
    "        section = raw.data[section_selector].squeeze()\n",
    "        interpolation = 3 if self.spec[self.intensities].interpolatable else 0\n",
    "        flow_x, flow_y, line_mask = self.deform_slice_transformations[c]\n",
    "        shape = section.shape\n",
    "        #做双线性插值\n",
    "        section = map_coordinates(\n",
    "            section, (flow_y, flow_x), mode='constant', order=interpolation\n",
    "        ).reshape(shape)\n",
    "        section = np.clip(section, 0., 1.)\n",
    "        section[line_mask] = 0\n",
    "        raw.data[section_selector] = section\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
